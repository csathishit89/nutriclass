# -*- coding: utf-8 -*-
"""NutriClass-RandomForest-PCA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15wnXiE2wcX8Qm8FKLS1dz2xrBvOQwSNt
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix
)

from sklearn.decomposition import PCA
from sklearn.ensemble import RandomForestClassifier

food_data_df = pd.read_csv('/content/synthetic_food_dataset_imbalanced.csv')
food_data_df.head()

food_data_df.shape

food_data_df.size

food_data_df.describe()

food_data_df.info()

## Feature wise Null value counts ##

food_data_df.isnull().sum()

## Total Duplicate values ##
food_data_df.duplicated().sum()

## Total Null values ##
food_data_df.isnull().sum().sum()

### To impute null values ###

nutrition_cols = [
    "Calories", "Protein", "Fat", "Carbs", "Sugar", "Fiber",
    "Sodium", "Cholesterol", "Glycemic_Index",
    "Water_Content", "Serving_Size"
]

food_data_df[nutrition_cols] = food_data_df[nutrition_cols].fillna(
    food_data_df[nutrition_cols].median()
)

cat_cols = ["Preparation_Method", "Food_Name", "Meal_Type"]
food_data_df[cat_cols] = food_data_df[cat_cols].fillna("Unknown")

bool_cols = ["Is_Vegan", "Is_Gluten_Free"]
for col in bool_cols:
    food_data_df[col].fillna(food_data_df[col].mode()[0])

processed_food_df = food_data_df.copy()

processed_food_df.drop_duplicates(inplace=True)

processed_food_df.duplicated().sum()

processed_food_df.isnull().sum().sum()

class_distribution = processed_food_df['Food_Name'].value_counts()
print("\nClass Distribution:")
print(class_distribution)

stats_per_class = processed_food_df.groupby('Food_Name')[nutrition_cols].mean()
print("\nMean feature values per class:")
print(stats_per_class)

### Class Distribution ###

plt.figure(figsize=(10, 6))
order = processed_food_df['Food_Name'].value_counts().index
sns.countplot(data=processed_food_df, x='Food_Name', order=order)
plt.title('Class Distribution (Food_Name)')
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('class_distribution.png')

### Calories Distribution ###

plt.figure(figsize=(12, 6))
sns.boxplot(data=processed_food_df, x='Food_Name', y='Calories', hue='Food_Name', palette='Set3', legend=False)
plt.title('Calories Distribution per Food Category')
plt.xticks(rotation=45)
plt.savefig('calories_boxplot.png')

# 3. Detect and cap outliers using IQR method
def cap_outliers(series):
    Q1 = series.quantile(0.25)
    Q3 = series.quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return series.clip(lower=lower_bound, upper=upper_bound)

df_capped = processed_food_df.copy()
for col in nutrition_cols:
    df_capped[col] = cap_outliers(processed_food_df[col])

# 4. Normalize or standardize numerical features
scaler = StandardScaler()
df_pre_processed = df_capped.copy()
df_pre_processed[nutrition_cols] = scaler.fit_transform(df_capped[nutrition_cols])

# Save the preprocessed dataset
output_file = 'preprocessed_food_data.csv'
df_pre_processed.to_csv(output_file, index=False)

"""**Preprocessed Dataset:**

1. Duplicate Removal

    A total of 313 duplicate rows were identified and removed to ensure data integrity.

2. Handling Missing Values

    Missing numerical values (which affected approximately $1.18\%$ of the dataset) were handled using class-specific median imputation. Instead of a global average, the median value for each specific Food_Name was used to fill NaNs, preserving the nutritional characteristics of each category.

3. Outlier Management

    Outliers in numerical features were addressed using the Interquartile Range (IQR) method. Values falling outside the boundaries of $[Q1 - 1.5 \times IQR, Q3 + 1.5 \times IQR]$ were capped (clipped) to these bounds. This reduces the impact of extreme values on model training while retaining the information from those observations.

4. Feature Standardization
  
    All numerical features were standardized using Z-score normalization (StandardScaler). This scales the features such that they have a mean of $0$ and a standard deviation of $1$, which is essential for many machine learning algorithms (e.g., SVMs, Neural Networks).
  
5. Final Dataset Structure

    The cleaned and transformed dataset contains 31,387 entries.
"""

## Identify Features & Target

X = df_pre_processed.drop("Food_Name", axis=1)
y = df_pre_processed["Food_Name"]

## Separate Numerical & Categorical Columns

num_cols = X.select_dtypes(include=["int64", "float64"]).columns
cat_cols = X.select_dtypes(include=["object", "bool"]).columns

## Preprocessing Pipeline

numeric_pipeline = Pipeline([
    ("scaler", StandardScaler())
])
numeric_pipeline

categorical_pipeline = Pipeline([
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])
categorical_pipeline

## Combine Using ColumnTransformer ##

preprocessor = ColumnTransformer([
    ("num", numeric_pipeline, num_cols),
    ("cat", categorical_pipeline, cat_cols)
])
preprocessor

## Train-Test Split ##

X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    stratify=y,
    random_state=42
)

## Random Forest Classifier Model Used for ML ##

model = RandomForestClassifier(
    n_estimators=200,
    class_weight="balanced",
    random_state=42
)
model

## Full ML Pipeline ##

pipeline = Pipeline([
    ("preprocessor", preprocessor),
    ("model", model)
])
pipeline

## Train Model ##

pipeline.fit(X_train, y_train)

## Model Evaluation ##

y_pred_train=pipeline.predict(X_train)
y_pred_test=pipeline.predict(X_test)

## accuracy check process ##

accuracy = accuracy_score(y_test, y_pred_test)
accuracy

# Evaluate the Model For Both Train And Test
print('Random Forest - Classification Report For Train')
print(classification_report(y_train,y_pred_train))
print('Random Forest - Classification Report For Test')
print(classification_report(y_test,y_pred_test))

## Confusion Matrix ##

confusion_matrix(y_test, y_pred_test)

## Preprocessing + PCA Pipeline ##

numeric_pipeline = Pipeline([
    ("scaler", StandardScaler()),
    ("pca", PCA(n_components=0.95))
])

categorical_pipeline = Pipeline([
    ("encoder", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor_pca = ColumnTransformer([
    ("num", numeric_pipeline, num_cols),
    ("cat", categorical_pipeline, cat_cols)
])

pipeline_pca = Pipeline([
    ("preprocessor", preprocessor_pca),
    ("model", RandomForestClassifier(
        n_estimators=200,
        class_weight="balanced",
        random_state=42
    ))
])

pipeline_pca.fit(X_train, y_train)
y_pred_pca = pipeline_pca.predict(X_test)

# Evaluate the Model For Both Train And Test
print('PCA - Report For Train Test')
print(classification_report(y_test, y_pred_pca))